#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸ‹ç‚¹æ•°æ®åˆ†æå’Œç»Ÿè®¡åŠŸèƒ½
æä¾›å„ç§æ•°æ®åˆ†æå’Œå¯è§†åŒ–åŠŸèƒ½
"""

from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import json
import logging
from tracking_models import TrackingDatabase

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TrackingAnalytics:
    """åŸ‹ç‚¹æ•°æ®åˆ†æç±»"""
    
    def __init__(self, db: TrackingDatabase):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            db: æ•°æ®åº“è¿æ¥å®ä¾‹
        """
        self.db = db
    
    def get_user_behavior_summary(self, days: int = 7) -> Dict[str, Any]:
        """
        è·å–ç”¨æˆ·è¡Œä¸ºæ‘˜è¦
        
        Args:
            days: åˆ†æå¤©æ•°
            
        Returns:
            Dict[str, Any]: ç”¨æˆ·è¡Œä¸ºæ‘˜è¦
        """
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            # è·å–äº‹ä»¶ç»Ÿè®¡
            event_stats = self.db.get_event_statistics(start_date, end_date)
            
            # è·å–é¡µé¢ç»Ÿè®¡
            page_stats = self.db.get_page_statistics(start_date, end_date)
            
            # è®¡ç®—ç”¨æˆ·æ´»è·ƒåº¦
            active_users = self._get_active_users(start_date, end_date)
            
            # è®¡ç®—ä¼šè¯ç»Ÿè®¡
            session_stats = self._get_session_statistics(start_date, end_date)
            
            return {
                'period': {
                    'start_date': start_date.isoformat(),
                    'end_date': end_date.isoformat(),
                    'days': days
                },
                'overview': {
                    'total_events': event_stats.get('total_events', 0),
                    'total_page_views': sum(p['views'] for p in page_stats.get('pages', [])),
                    'active_users': active_users['unique_users'],
                    'total_sessions': session_stats['total_sessions']
                },
                'event_types': event_stats.get('event_types', []),
                'top_pages': page_stats.get('pages', [])[:10],
                'user_activity': active_users,
                'session_metrics': session_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç”¨æˆ·è¡Œä¸ºæ‘˜è¦å¤±è´¥: {e}")
            return {}
    
    def _get_active_users(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """è·å–æ´»è·ƒç”¨æˆ·ç»Ÿè®¡"""
        try:
            pipeline = [
                {
                    "$match": {
                        "timestamp": {
                            "$gte": start_date,
                            "$lte": end_date
                        },
                        "user_id": {"$ne": None}
                    }
                },
                {
                    "$group": {
                        "_id": "$user_id",
                        "event_count": {"$sum": 1},
                        "last_activity": {"$max": "$timestamp"},
                        "sessions": {"$addToSet": "$session_id"}
                    }
                },
                {
                    "$project": {
                        "user_id": "$_id",
                        "event_count": 1,
                        "last_activity": 1,
                        "session_count": {"$size": "$sessions"}
                    }
                }
            ]
            
            users = list(self.db.events_collection.aggregate(pipeline))
            
            return {
                'unique_users': len(users),
                'users': users,
                'avg_events_per_user': sum(u['event_count'] for u in users) / len(users) if users else 0
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ´»è·ƒç”¨æˆ·ç»Ÿè®¡å¤±è´¥: {e}")
            return {'unique_users': 0, 'users': [], 'avg_events_per_user': 0}
    
    def _get_session_statistics(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """è·å–ä¼šè¯ç»Ÿè®¡"""
        try:
            pipeline = [
                {
                    "$match": {
                        "timestamp": {
                            "$gte": start_date,
                            "$lte": end_date
                        }
                    }
                },
                {
                    "$group": {
                        "_id": "$session_id",
                        "user_id": {"$first": "$user_id"},
                        "start_time": {"$min": "$timestamp"},
                        "end_time": {"$max": "$timestamp"},
                        "event_count": {"$sum": 1},
                        "page_views": {
                            "$sum": {
                                "$cond": [{"$eq": ["$event_type", "page_view"]}, 1, 0]
                            }
                        }
                    }
                },
                {
                    "$project": {
                        "session_id": "$_id",
                        "user_id": 1,
                        "start_time": 1,
                        "end_time": 1,
                        "event_count": 1,
                        "page_views": 1,
                        "duration_minutes": {
                            "$divide": [
                                {"$subtract": ["$end_time", "$start_time"]},
                                60000
                            ]
                        }
                    }
                }
            ]
            
            sessions = list(self.db.events_collection.aggregate(pipeline))
            
            if not sessions:
                return {
                    'total_sessions': 0,
                    'avg_duration_minutes': 0,
                    'avg_events_per_session': 0,
                    'avg_page_views_per_session': 0
                }
            
            durations = [s['duration_minutes'] for s in sessions if s['duration_minutes'] > 0]
            
            return {
                'total_sessions': len(sessions),
                'avg_duration_minutes': sum(durations) / len(durations) if durations else 0,
                'avg_events_per_session': sum(s['event_count'] for s in sessions) / len(sessions),
                'avg_page_views_per_session': sum(s['page_views'] for s in sessions) / len(sessions),
                'sessions': sessions
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–ä¼šè¯ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                'total_sessions': 0,
                'avg_duration_minutes': 0,
                'avg_events_per_session': 0,
                'avg_page_views_per_session': 0
            }
    
    def get_funnel_analysis(self, funnel_steps: List[str], days: int = 7) -> Dict[str, Any]:
        """
        è·å–æ¼æ–—åˆ†æ
        
        Args:
            funnel_steps: æ¼æ–—æ­¥éª¤åˆ—è¡¨ï¼ŒæŒ‰é¡ºåºæ’åˆ—
            days: åˆ†æå¤©æ•°
            
        Returns:
            Dict[str, Any]: æ¼æ–—åˆ†æç»“æœ
        """
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            # è·å–æ¯ä¸ªæ­¥éª¤çš„ç”¨æˆ·æ•°
            step_counts = []
            for i, step in enumerate(funnel_steps):
                pipeline = [
                    {
                        "$match": {
                            "timestamp": {"$gte": start_date, "$lte": end_date},
                            "event_type": step
                        }
                    },
                    {
                        "$group": {
                            "_id": "$user_id",
                            "count": {"$sum": 1}
                        }
                    },
                    {
                        "$count": "unique_users"
                    }
                ]
                
                result = list(self.db.events_collection.aggregate(pipeline))
                count = result[0]['unique_users'] if result else 0
                step_counts.append({
                    'step': step,
                    'users': count,
                    'conversion_rate': 0  # å°†åœ¨ä¸‹é¢è®¡ç®—
                })
            
            # è®¡ç®—è½¬åŒ–ç‡
            for i in range(len(step_counts)):
                if i == 0:
                    step_counts[i]['conversion_rate'] = 100.0
                else:
                    previous_users = step_counts[i-1]['users']
                    current_users = step_counts[i]['users']
                    if previous_users > 0:
                        step_counts[i]['conversion_rate'] = (current_users / previous_users) * 100
                    else:
                        step_counts[i]['conversion_rate'] = 0.0
            
            return {
                'funnel_steps': step_counts,
                'overall_conversion': step_counts[-1]['conversion_rate'] if step_counts else 0,
                'period': {
                    'start_date': start_date.isoformat(),
                    'end_date': end_date.isoformat(),
                    'days': days
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ æ¼æ–—åˆ†æå¤±è´¥: {e}")
            return {}
    
    def get_retention_analysis(self, days: int = 30) -> Dict[str, Any]:
        """
        è·å–ç”¨æˆ·ç•™å­˜åˆ†æ
        
        Args:
            days: åˆ†æå¤©æ•°
            
        Returns:
            Dict[str, Any]: ç•™å­˜åˆ†æç»“æœ
        """
        try:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            # è·å–ç”¨æˆ·é¦–æ¬¡è®¿é—®æ—¶é—´
            first_visit_pipeline = [
                {
                    "$match": {
                        "timestamp": {"$gte": start_date, "$lte": end_date},
                        "user_id": {"$ne": None}
                    }
                },
                {
                    "$group": {
                        "_id": "$user_id",
                        "first_visit": {"$min": "$timestamp"}
                    }
                }
            ]
            
            first_visits = list(self.db.events_collection.aggregate(first_visit_pipeline))
            first_visit_dict = {v['_id']: v['first_visit'] for v in first_visits}
            
            # è®¡ç®—ç•™å­˜ç‡
            retention_data = []
            for day in range(1, 8):  # 1-7å¤©ç•™å­˜
                cohort_users = []
                retained_users = []
                
                for user_id, first_visit in first_visit_dict.items():
                    cohort_date = first_visit.date()
                    if cohort_date >= (end_date - timedelta(days=days-day)).date():
                        cohort_users.append(user_id)
                        
                        # æ£€æŸ¥ç”¨æˆ·åœ¨é¦–æ¬¡è®¿é—®åç¬¬Nå¤©æ˜¯å¦æ´»è·ƒ
                        check_date = first_visit + timedelta(days=day)
                        check_start = check_date.replace(hour=0, minute=0, second=0, microsecond=0)
                        check_end = check_start + timedelta(days=1)
                        
                        user_events = self.db.events_collection.find({
                            "user_id": user_id,
                            "timestamp": {"$gte": check_start, "$lt": check_end}
                        })
                        
                        if list(user_events):
                            retained_users.append(user_id)
                
                retention_rate = (len(retained_users) / len(cohort_users) * 100) if cohort_users else 0
                retention_data.append({
                    'day': day,
                    'cohort_size': len(cohort_users),
                    'retained_users': len(retained_users),
                    'retention_rate': retention_rate
                })
            
            return {
                'retention_data': retention_data,
                'period': {
                    'start_date': start_date.isoformat(),
                    'end_date': end_date.isoformat(),
                    'days': days
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ç•™å­˜åˆ†æå¤±è´¥: {e}")
            return {}
    
    def generate_visualization_report(self, days: int = 7, output_dir: str = 'analytics_reports') -> str:
        """
        ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        
        Args:
            days: åˆ†æå¤©æ•°
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        try:
            import os
            os.makedirs(output_dir, exist_ok=True)
            
            # è·å–æ•°æ®
            summary = self.get_user_behavior_summary(days)
            
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'ç”¨æˆ·è¡Œä¸ºåˆ†ææŠ¥å‘Š - æœ€è¿‘{days}å¤©', fontsize=16, fontweight='bold')
            
            # 1. äº‹ä»¶ç±»å‹åˆ†å¸ƒ
            if summary.get('event_types'):
                event_types = [e['event_type'] for e in summary['event_types']]
                event_counts = [e['count'] for e in summary['event_types']]
                
                axes[0, 0].pie(event_counts, labels=event_types, autopct='%1.1f%%')
                axes[0, 0].set_title('äº‹ä»¶ç±»å‹åˆ†å¸ƒ')
            
            # 2. é¡µé¢è®¿é—®TOP10
            if summary.get('top_pages'):
                pages = [p['page_url'][:30] + '...' if len(p['page_url']) > 30 else p['page_url'] 
                        for p in summary['top_pages'][:10]]
                views = [p['views'] for p in summary['top_pages'][:10]]
                
                axes[0, 1].barh(range(len(pages)), views)
                axes[0, 1].set_yticks(range(len(pages)))
                axes[0, 1].set_yticklabels(pages)
                axes[0, 1].set_title('é¡µé¢è®¿é—®TOP10')
                axes[0, 1].set_xlabel('è®¿é—®æ¬¡æ•°')
            
            # 3. ç”¨æˆ·æ´»è·ƒåº¦è¶‹åŠ¿ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
            dates = [(datetime.now() - timedelta(days=i)).strftime('%m-%d') for i in range(days-1, -1, -1)]
            # è¿™é‡Œåº”è¯¥ä»å®é™…æ•°æ®ä¸­è·å–æ¯æ—¥æ´»è·ƒç”¨æˆ·æ•°
            daily_active_users = [summary['overview']['active_users'] // days] * days
            
            axes[1, 0].plot(dates, daily_active_users, marker='o')
            axes[1, 0].set_title('æ¯æ—¥æ´»è·ƒç”¨æˆ·æ•°')
            axes[1, 0].set_xlabel('æ—¥æœŸ')
            axes[1, 0].set_ylabel('æ´»è·ƒç”¨æˆ·æ•°')
            axes[1, 0].tick_params(axis='x', rotation=45)
            
            # 4. å…³é”®æŒ‡æ ‡
            metrics = [
                f"æ€»äº‹ä»¶æ•°: {summary['overview']['total_events']:,}",
                f"é¡µé¢è®¿é—®: {summary['overview']['total_page_views']:,}",
                f"æ´»è·ƒç”¨æˆ·: {summary['overview']['active_users']:,}",
                f"æ€»ä¼šè¯æ•°: {summary['overview']['total_sessions']:,}"
            ]
            
            axes[1, 1].text(0.1, 0.8, '\n'.join(metrics), transform=axes[1, 1].transAxes,
                           fontsize=12, verticalalignment='top',
                           bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
            axes[1, 1].set_title('å…³é”®æŒ‡æ ‡')
            axes[1, 1].axis('off')
            
            plt.tight_layout()
            
            # ä¿å­˜æŠ¥å‘Š
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_path = os.path.join(output_dir, f'analytics_report_{timestamp}.png')
            plt.savefig(report_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"âœ… å¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            return report_path
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šå¤±è´¥: {e}")
            return ""
    
    def export_data_to_csv(self, days: int = 7, output_dir: str = 'analytics_reports') -> Dict[str, str]:
        """
        å¯¼å‡ºæ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            days: åˆ†æå¤©æ•°
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Dict[str, str]: å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
        """
        try:
            import os
            os.makedirs(output_dir, exist_ok=True)
            
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            exported_files = {}
            
            # å¯¼å‡ºäº‹ä»¶æ•°æ®
            events = self.db.get_events_by_date_range(start_date, end_date)
            if events:
                events_df = pd.DataFrame(events)
                events_file = os.path.join(output_dir, f'events_{timestamp}.csv')
                events_df.to_csv(events_file, index=False, encoding='utf-8-sig')
                exported_files['events'] = events_file
            
            # å¯¼å‡ºé¡µé¢è®¿é—®æ•°æ®
            pageviews = self.db.get_page_views_by_date_range(start_date, end_date)
            if pageviews:
                pageviews_df = pd.DataFrame(pageviews)
                pageviews_file = os.path.join(output_dir, f'pageviews_{timestamp}.csv')
                pageviews_df.to_csv(pageviews_file, index=False, encoding='utf-8-sig')
                exported_files['pageviews'] = pageviews_file
            
            logger.info(f"âœ… æ•°æ®å¯¼å‡ºå®Œæˆ: {exported_files}")
            return exported_files
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
            return {}
    
    def get_real_time_stats(self) -> Dict[str, Any]:
        """
        è·å–å®æ—¶ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: å®æ—¶ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            now = datetime.now()
            last_hour = now - timedelta(hours=1)
            last_24h = now - timedelta(hours=24)
            
            # æœ€è¿‘1å°æ—¶äº‹ä»¶æ•°
            recent_events = self.db.events_collection.count_documents({
                "timestamp": {"$gte": last_hour}
            })
            
            # æœ€è¿‘24å°æ—¶äº‹ä»¶æ•°
            daily_events = self.db.events_collection.count_documents({
                "timestamp": {"$gte": last_24h}
            })
            
            # å½“å‰åœ¨çº¿ç”¨æˆ·ï¼ˆæœ€è¿‘5åˆ†é’Ÿæœ‰æ´»åŠ¨çš„ç”¨æˆ·ï¼‰
            online_threshold = now - timedelta(minutes=5)
            online_users = len(self.db.events_collection.distinct("user_id", {
                "timestamp": {"$gte": online_threshold},
                "user_id": {"$ne": None}
            }))
            
            return {
                'timestamp': now.isoformat(),
                'recent_events_1h': recent_events,
                'recent_events_24h': daily_events,
                'online_users': online_users,
                'status': 'healthy' if self.db else 'unhealthy'
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–å®æ—¶ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                'timestamp': datetime.now().isoformat(),
                'recent_events_1h': 0,
                'recent_events_24h': 0,
                'online_users': 0,
                'status': 'error',
                'error': str(e)
            }

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºåˆ†æåŠŸèƒ½"""
    try:
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db = TrackingDatabase()
        analytics = TrackingAnalytics(db)
        
        print("ğŸ” å¼€å§‹æ•°æ®åˆ†æ...")
        
        # è·å–ç”¨æˆ·è¡Œä¸ºæ‘˜è¦
        summary = analytics.get_user_behavior_summary(days=7)
        print(f"ğŸ“Š ç”¨æˆ·è¡Œä¸ºæ‘˜è¦: {json.dumps(summary, indent=2, ensure_ascii=False)}")
        
        # è·å–å®æ—¶ç»Ÿè®¡
        realtime = analytics.get_real_time_stats()
        print(f"â° å®æ—¶ç»Ÿè®¡: {json.dumps(realtime, indent=2, ensure_ascii=False)}")
        
        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        report_path = analytics.generate_visualization_report(days=7)
        if report_path:
            print(f"ğŸ“ˆ å¯è§†åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # å¯¼å‡ºæ•°æ®
        exported_files = analytics.export_data_to_csv(days=7)
        if exported_files:
            print(f"ğŸ“ æ•°æ®å·²å¯¼å‡º: {exported_files}")
        
        db.close()
        
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")

if __name__ == '__main__':
    main()